{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate train, val and test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dataset\n",
    "    - MIDV500\n",
    "        - Contains 50 different identity document types, 10 different condition and devices in each document type\n",
    "        - Condition\n",
    "            - Table\n",
    "            - Keyboard\n",
    "            - Hand\n",
    "            - Partial\n",
    "            - Clutter\n",
    "        - Device\n",
    "            - Iphone 5\n",
    "            - Samsung S3\n",
    "    - MIDV2019\n",
    "        - Contains 50 different identity document types, provide 4 extra different condition and devices.\n",
    "        - Condition\n",
    "            - Distorted\n",
    "            - Low lightning\n",
    "        - Device\n",
    "            - Iphone XS Max\n",
    "            - Samsung S10\n",
    "- How to split into train / val / test dataset\n",
    "    - To make sure the generalization ability of model and consistency of distribution between train, val and test dataset, the strategies that we will use to split dataset are listed as follow:\n",
    "        1. Split **midv500** into **midv500-train**, **midv500-val** and **midv500-test** with euqally frequency of different combination of condition, device and document type.\n",
    "        2. Do the same thing in step 1 in **midv2019** dataset.\n",
    "        3. Merge **midv500-train** and **midv2019-train** into **midv-train**, so are val and test dataset.\n",
    "    - Note: the ration between train, val and test is 60%, 20% and 20%, respectivtly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function definitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: the order of ratio is: (train, val, test)\n",
    "def get_split_index(annotation: dict, ratio: list):\n",
    "    index_map = {}\n",
    "\n",
    "    # Record the idx for each combination, (document type, condition and device)\n",
    "    for idx, image in enumerate(annotation[\"images\"]):\n",
    "        s = image[\"file_name\"].split(\"/\")\n",
    "        s = s[0][:2] + s[2]\n",
    "        if s in index_map:\n",
    "            index_map[s].append(idx)\n",
    "        else:\n",
    "            index_map[s] = [idx]\n",
    "\n",
    "    # Split into train, val and test set with given ratio\n",
    "    index_map_all = {\n",
    "        \"train\": [],\n",
    "        \"val\": [],\n",
    "        \"test\": [],\n",
    "    }\n",
    "    for key, value in index_map.items():\n",
    "        random.shuffle(value)\n",
    "\n",
    "        size = np.array(ratio) * len(value)\n",
    "        train_len, val_len, test_len = size.astype(int)\n",
    "        indexes = {\n",
    "            \"train\": value[:train_len],\n",
    "            \"val\": value[train_len : (train_len + val_len)],\n",
    "            \"test\": value[(train_len + val_len) :],\n",
    "        }\n",
    "\n",
    "        for cat in index_map_all.keys():\n",
    "            index_map_all[cat].extend(indexes[cat])\n",
    "    return index_map_all\n",
    "\n",
    "\n",
    "def split_annotation(annotation: dict, index_map: dict):\n",
    "    images = np.array(annotation[\"images\"])\n",
    "    anno = np.array(annotation[\"annotations\"])\n",
    "    categories = annotation[\"categories\"]\n",
    "\n",
    "    annotation_map = {}\n",
    "    for key, value in index_map.items():\n",
    "        annotation_map[key] = {\n",
    "            \"images\": list(images[value]),\n",
    "            \"annotations\": list(anno[value]),\n",
    "            \"categories\": categories,\n",
    "        }\n",
    "    return annotation_map\n",
    "\n",
    "\n",
    "def merge_annotation(anno_A, anno_B):\n",
    "    merge_anno = {key: value[:] for key, value in anno_A.items()}\n",
    "\n",
    "    idx = [image[\"id\"] for image in merge_anno[\"images\"]]\n",
    "    shift = max(idx) + 1\n",
    "\n",
    "    for idx, val in enumerate(anno_B[\"annotations\"]):\n",
    "        anno_B[\"annotations\"][idx][\"image_id\"] += shift\n",
    "        anno_B[\"annotations\"][idx][\"id\"] += shift\n",
    "\n",
    "    for idx, val in enumerate(anno_B[\"images\"]):\n",
    "        anno_B[\"images\"][idx][\"id\"] += shift\n",
    "\n",
    "    for key in [\"annotations\", \"images\"]:\n",
    "        merge_anno[key].extend(anno_B[key])\n",
    "\n",
    "    return merge_anno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/card-segmentation/midv/midv500_coco.json\", \"r\") as f:\n",
    "    midv500_anno = json.load(f)\n",
    "\n",
    "with open(\"/data/card-segmentation/midv/midv2019_coco.json\", \"r\") as f:\n",
    "    midv2019_anno = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split midv500 and midv2019 into train, val and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = [0.6, 0.2, 0.2]\n",
    "\n",
    "index_map = get_split_index(midv500_anno, sampling_rate)\n",
    "midv500_anno_split = split_annotation(midv500_anno, index_map)\n",
    "\n",
    "\n",
    "index_map = get_split_index(midv2019_anno, sampling_rate)\n",
    "midv2019_anno_split = split_annotation(midv2019_anno, index_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_types = [\"train\", \"val\", \"test\"]\n",
    "merge_annotations = {}\n",
    "\n",
    "for key in dataset_types:\n",
    "    merge_annotations[key] = merge_annotation(\n",
    "        midv500_anno_split[key], midv2019_anno_split[key]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify the merge result by sample siez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "midv500\n",
      "k: train, len: 8840\n",
      "k: val, len: 2939\n",
      "k: test, len: 2969\n",
      "\n",
      "\n",
      "midv2019\n",
      "k: train, len: 3488\n",
      "k: val, len: 1154\n",
      "k: test, len: 1189\n",
      "\n",
      "\n",
      "merge_result\n",
      "k: train, len: 12328\n",
      "k: val, len: 4093\n",
      "k: test, len: 4158\n"
     ]
    }
   ],
   "source": [
    "print(\"midv500\")\n",
    "for k, v in midv500_anno_split.items():\n",
    "    print(\"k: {}, len: {}\".format(k, len(v[\"images\"])))\n",
    "\n",
    "print(\"\\n\\nmidv2019\")\n",
    "for k, v in midv2019_anno_split.items():\n",
    "    print(\"k: {}, len: {}\".format(k, len(v[\"images\"])))\n",
    "\n",
    "\n",
    "print(\"\\n\\nmerge_result\")\n",
    "for k, v in merge_annotations.items():\n",
    "    print(\"k: {}, len: {}\".format(k, len(v[\"images\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"/data/card-segmentation/midv/\"\n",
    "\n",
    "for key, value in merge_annotations.items():\n",
    "    with open(f\"{output_path}midv_{key}.json\", \"w\") as outfile:\n",
    "        json.dump(value, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
